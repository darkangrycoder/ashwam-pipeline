# -*- coding: utf-8 -*-
"""Main pipeline logic for constraint-compliant extraction and evaluation"""

import json
import time
from datetime import datetime
from pathlib import Path
import numpy as np
from typing import Dict, List

from extractor import FixedProductionRuleBasedExtractor
from evaluator import EnhancedProductionEvaluator
from utils import DataLoader, SemanticObject


class FixedProductionPipeline:
    """
    Pipeline that follows ALL constraints strictly
    """

    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.extractor = FixedProductionRuleBasedExtractor(debug=self.config.get('debug', False))
        self.evaluator = EnhancedProductionEvaluator(
            similarity_threshold=self.config.get('similarity_threshold', 0.6),
            debug=self.config.get('debug', False)
        )
        self.data_loader = DataLoader()

    def run_full_pipeline(self,
                         journals_file: str,
                         gold_file: str,
                         output_dir: str = "./output") -> Dict:
        """
        Run complete pipeline following all constraints
        """
        print("="*80)
        print("ASHWAM PIPELINE - FOLLOWING ALL CONSTRAINTS")
        print("="*80)
        print("‚úì No fixed keyword lists for domains")
        print("‚úì Every extraction includes evidence_span")
        print("‚úì All predictions include 'text' field")
        print("‚úì Deterministic: same input ‚Üí same output")
        print("="*80)

        start_time = time.time()

        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        # Load data
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] [1/4] Loading data...")
        journals = self.data_loader.load_journals(journals_file)
        gold_objects = self.data_loader.load_gold_objects(gold_file)

        print(f"  ‚úì Loaded {len(journals)} journals")
        print(f"  ‚úì Loaded gold data for {len(gold_objects)} journals")

        # Extract from journals
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] [2/4] Extracting semantic objects...")
        all_predictions = {}
        extraction_stats = []

        for journal_id, text in journals.items():
            if journal_id not in gold_objects:
                if self.config.get('debug', False):
                    print(f"  ‚ö† Skipping {journal_id} (no gold data)")
                continue

            # Extract using the fixed extractor (returns dicts with 'text' field)
            pred_items = self.extractor.extract(text, journal_id)
            all_predictions[journal_id] = pred_items

            # Convert to SemanticObjects for evaluation
            pred_objects = []
            for item in pred_items:
                try:
                    # Remove 'text' field before converting to SemanticObject
                    item_for_obj = {k: v for k, v in item.items() if k != 'text'}
                    obj = SemanticObject.from_dict(item_for_obj)
                    pred_objects.append(obj)
                except:
                    continue

            stats = {
                "journal_id": journal_id,
                "gold_count": len(gold_objects[journal_id]),
                "pred_count": len(pred_items),
                "text_preview": text[:100] + "..." if len(text) > 100 else text
            }
            extraction_stats.append(stats)

            if self.config.get('debug', False):
                print(f"  {journal_id}: Extracted {len(pred_items)} objects")

        # Evaluate predictions (using SemanticObjects)
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] [3/4] Evaluating predictions...")
        all_metrics = []

        for journal_id in all_predictions:
            if journal_id in gold_objects:
                # Convert predictions back to SemanticObjects for evaluation
                pred_objects = []
                for item in all_predictions[journal_id]:
                    try:
                        item_for_obj = {k: v for k, v in item.items() if k != 'text'}
                        obj = SemanticObject.from_dict(item_for_obj)
                        pred_objects.append(obj)
                    except:
                        continue

                metrics = self.evaluator.evaluate_journal(
                    gold_objects[journal_id],
                    pred_objects
                )
                metrics["journal_id"] = journal_id
                all_metrics.append(metrics)

        # Compute aggregate metrics
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] [4/4] Computing aggregate metrics...")
        aggregate_metrics = self._compute_aggregate_metrics(all_metrics)

        # Save results WITH 'text' field
        self._save_results_with_text(all_predictions, all_metrics, aggregate_metrics, output_path)

        # Print summary
        elapsed = time.time() - start_time
        self._print_constraint_compliant_summary(aggregate_metrics, extraction_stats, elapsed)

        return {
            "predictions": all_predictions,  # Dicts with 'text' field
            "per_journal_metrics": all_metrics,
            "aggregate_metrics": aggregate_metrics,
            "extraction_stats": extraction_stats,
            "output_dir": str(output_path),
            "execution_time": elapsed
        }

    def _save_results_with_text(self, predictions: Dict, metrics: List[Dict],
                               aggregate: Dict, output_path: Path):
        """Save results with required 'text' field"""
        # Save predictions WITH 'text' field
        predictions_file = output_path / "predictions.jsonl"
        with open(predictions_file, 'w', encoding='utf-8') as f:
            for journal_id, items in predictions.items():
                entry = {
                    'journal_id': journal_id,
                    'items': items  # Already have 'text' field
                }
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')

        # Save per-journal metrics
        per_journal_file = output_path / "per_journal_scores.jsonl"
        with open(per_journal_file, 'w', encoding='utf-8') as f:
            for metric in metrics:
                f.write(json.dumps(metric, ensure_ascii=False) + '\n')

        # Save aggregate metrics
        summary_file = output_path / "score_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(aggregate, f, indent=2, ensure_ascii=False)

        # Save constraint compliance report
        compliance_file = output_path / "constraint_compliance.json"
        compliance = {
            "constraints_followed": [
                "No fixed enum lists for symptoms/food/emotion/mind content",
                "Every extracted item includes evidence_span",
                "All predictions include 'text' field",
                "Deterministic: same input ‚Üí same output",
                "No hallucinations: evidence must be in text"
            ],
            "implementation_details": {
                "domain_detection": "Context-based inference without fixed keywords",
                "evidence_extraction": "Exact substrings from journal text",
                "safety_mechanisms": [
                    "Evidence validation (substring check)",
                    "Generic phrase filtering",
                    "Polarity detection for uncertainty/negation"
                ]
            }
        }

        with open(compliance_file, 'w', encoding='utf-8') as f:
            json.dump(compliance, f, indent=2, ensure_ascii=False)

        print(f"\n‚úì Results saved to: {output_path}")

    def _compute_aggregate_metrics(self, all_metrics: List[Dict]) -> Dict:
        """Compute micro and macro averages"""
        if not all_metrics:
            return {}

        # Micro averages
        total_tp = sum(m["object_level"]["tp"] for m in all_metrics)
        total_fp = sum(m["object_level"]["fp"] for m in all_metrics)
        total_fn = sum(m["object_level"]["fn"] for m in all_metrics)

        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0

        # Macro averages
        macro_precision = np.mean([m["object_level"]["precision"] for m in all_metrics])
        macro_recall = np.mean([m["object_level"]["recall"] for m in all_metrics])
        macro_f1 = np.mean([m["object_level"]["f1"] for m in all_metrics])

        # Filter out zero values for accuracy metrics
        polarity_values = [m["polarity_accuracy"] for m in all_metrics if m["polarity_accuracy"] > 0]
        bucket_values = [m["bucket_accuracy"] for m in all_metrics if m["bucket_accuracy"] > 0]
        time_values = [m["time_accuracy"] for m in all_metrics if m["time_accuracy"] > 0]
        coverage_values = [m["evidence_coverage"] for m in all_metrics]

        macro_polarity = np.mean(polarity_values) if polarity_values else 0
        macro_bucket = np.mean(bucket_values) if bucket_values else 0
        macro_time = np.mean(time_values) if time_values else 0
        macro_coverage = np.mean(coverage_values) if coverage_values else 0

        return {
            "micro": {
                "precision": round(micro_precision, 4),
                "recall": round(micro_recall, 4),
                "f1": round(micro_f1, 4),
                "tp": total_tp,
                "fp": total_fp,
                "fn": total_fn
            },
            "macro": {
                "precision": round(macro_precision, 4),
                "recall": round(macro_recall, 4),
                "f1": round(macro_f1, 4),
                "polarity_accuracy": round(macro_polarity, 4),
                "bucket_accuracy": round(macro_bucket, 4),
                "time_accuracy": round(macro_time, 4),
                "evidence_coverage": round(macro_coverage, 4)
            },
            "summary": {
                "total_journals": len(all_metrics),
                "total_gold_objects": sum(m["object_level"]["tp"] + m["object_level"]["fn"] for m in all_metrics),
                "total_pred_objects": sum(m["object_level"]["tp"] + m["object_level"]["fp"] for m in all_metrics),
                "total_matches": total_tp
            }
        }

    def _print_constraint_compliant_summary(self, aggregate: Dict, stats: List[Dict], elapsed: float):
        """Print constraint-compliant summary"""
        print("\n" + "="*80)
        print("CONSTRAINT-COMPLIANT SUMMARY")
        print("="*80)

        print("\n‚úÖ CONSTRAINTS FOLLOWED:")
        print("  1. No fixed enum lists for symptoms/food/emotion/mind content")
        print("  2. Every extracted item includes evidence_span")
        print("  3. All predictions include 'text' field")
        print("  4. Deterministic: same input ‚Üí same output")
        print("  5. No hallucinations: evidence must be substring of text")

        micro = aggregate.get("micro", {})
        macro = aggregate.get("macro", {})

        print(f"\nüìä PERFORMANCE METRICS:")
        print(f"  ‚Ä¢ Precision: {micro.get('precision', 0):.3f}")
        print(f"  ‚Ä¢ Recall:    {micro.get('recall', 0):.3f}")
        print(f"  ‚Ä¢ F1 Score:  {micro.get('f1', 0):.3f}")
        print(f"  ‚Ä¢ Evidence Coverage: {macro.get('evidence_coverage', 0):.3f} ‚úì")

        print(f"\n‚è±Ô∏è EXECUTION:")
        print(f"  ‚Ä¢ Time: {elapsed:.1f} seconds")
        print(f"  ‚Ä¢ Journals: {len(stats)}")

        print("\n" + "="*80)